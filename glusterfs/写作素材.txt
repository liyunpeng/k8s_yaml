[master@212-node glusterfs]$ k get po -owide
NAME                             READY   STATUS    RESTARTS   AGE     IP              NODE       NOMINATED NODE   READINESS GATES
deploy-heketi-65d9c564d6-69dq6   1/1     Running   0          3h31m   192.168.135.1   217-node   <none>           <none>
glusterfs-pfqdr                  0/1     Running   56         3h33m   192.168.0.213   213-node   <none>           <none>
glusterfs-vhbgn                  1/1     Running   0          3h33m   192.168.0.217   217-node   <none>           <none>

glusterfs-pfqdr 重启了56次
[master@212-node glusterfs]$ k logs glusterfs-pfqdr
env variable is set. Update in gluster-blockd.service
原因


[master@212-node glusterfs]$ kd po glusterfs-pfqdr
Name:         glusterfs-pfqdr
Namespace:    default
Priority:     0
Node:         213-node/192.168.0.213
Start Time:   Sat, 11 Jan 2020 12:54:35 +0800
Labels:       app=filebeat
              controller-revision-hash=84bb7bd44
              glusterfs=pod
              glusterfs-node=pod
              pod-template-generation=1
Annotations:  <none>
Status:       Running
IP:           192.168.0.213
IPs:
  IP:           192.168.0.213
Controlled By:  DaemonSet/glusterfs
Containers:
  glusterfs:
    Container ID:   docker://8df8f66e3798842ae4dcff0548297908f9a77cfaa7dab5fe3e33b0cf6264f15d
    Image:          gluster/gluster-centos:latest
    Image ID:       docker-pullable://gluster/gluster-centos@sha256:124e2b81c245af24e543d76606ed4983c7b6dfbbca7031b77bb53779fe1c04f2
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sat, 11 Jan 2020 16:26:59 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Sat, 11 Jan 2020 16:24:59 +0800
      Finished:     Sat, 11 Jan 2020 16:26:58 +0800
    Ready:          False
    Restart Count:  56
    Liveness:       exec [/bin/bash -c systemctl status glusterd.service] delay=60s timeout=3s period=10s #success=1 #failure=3
    Readiness:      exec [/bin/bash -c systemctl status glusterd.service] delay=60s timeout=3s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /dev from glusterfs-dev (rw)
      /etc/glusterfs from glusterfs-etc (rw)
      /etc/ssl from glusterfs-ssl (ro)
      /run from glusterfs-run (rw)
      /run/lvm from glusterfs-lvm (rw)
      /sys/fs/cgroup from glusterfs-cgroup (ro)
      /var/lib/glusterd from glusterfs-config (rw)
      /var/lib/heketi from glusterfs-heketi (rw)
      /var/lib/misc/glusterfsd from glusterfs-misc (rw)
      /var/log/glusterfs from glusterfs-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-24p4w (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  glusterfs-heketi:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/heketi
    HostPathType:
  glusterfs-run:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  glusterfs-lvm:
    Type:          HostPath (bare host directory volume)
    Path:          /run/lvm
    HostPathType:
  glusterfs-etc:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/glusterfs
    HostPathType:
  glusterfs-logs:
    Type:          HostPath (bare host directory volume)
    Path:          /var/log/glusterfs
    HostPathType:
  glusterfs-config:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/glusterd
    HostPathType:
  glusterfs-dev:
    Type:          HostPath (bare host directory volume)
    Path:          /dev
    HostPathType:
  glusterfs-misc:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/misc/glusterfsd
    HostPathType:
  glusterfs-cgroup:
    Type:          HostPath (bare host directory volume)
    Path:          /sys/fs/cgroup
    HostPathType:
  glusterfs-ssl:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl
    HostPathType:
  default-token-24p4w:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-24p4w
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  storagenode=glusterfs
Tolerations:     node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason     Age                      From               Message
  ----     ------     ----                     ----               -------
  Normal   Pulled     48m (x45 over 3h30m)     kubelet, 213-node  Container image "gluster/gluster-centos:latest" already present on machine
  Warning  Unhealthy  8m42s (x485 over 3h29m)  kubelet, 213-node  (combined from similar events): Readiness probe failed: ● glusterd.service - GlusterFS, a clustered file-system server
   Loaded: loaded (/usr/lib/systemd/system/glusterd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Sat 2020-01-11 08:17:51 UTC; 1min 47s ago

Jan 11 08:17:49 213-node systemd[1]: Starting GlusterFS, a clustered file-system server...
Jan 11 08:17:51 213-node systemd[1]: glusterd.service: control process exited, code=exited status=1
Jan 11 08:17:51 213-node systemd[1]: Failed to start GlusterFS, a clustered file-system server.
Jan 11 08:17:51 213-node systemd[1]: Unit glusterd.service entered failed state.
Jan 11 08:17:51 213-node systemd[1]: glusterd.service failed.
  Warning  BackOff  3m34s (x477 over 176m)  kubelet, 213-node  Back-off restarting failed container
原因是213-node主机上没有启动glusterd.service服务

措施， 在213-node主机启动glusterd服务
[centos2@213-node ~]$ ps -e | grep glu
[centos2@213-node ~]$ sudo systemctl start glusterd.service

[centos2@213-node shell]$ ./catcpumem.sh
物理总内存=MemTotal: 1849464 kB
物理剩余内存=MemFree: 117852 kB
内核版本与操作系统=Linux version 4.18.0-80.el8.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 8.2.1 20180905 (Red Hat 8.2.1-3) (GCC)) #1 SMP Tue Jun 4 09:19:46 UTC 2019
cpu数目=4
一个cpu上的核数=cpu cores : 1
一个cpu上的核数和cpu型号和每个核的频率= 4 Intel(R) Core(TM) i5-2400 CPU @ 3.10GHz
修改虚拟机内存为3G
[centos2@213-node shell]$ ./catcpumem.sh
物理总内存=MemTotal: 2898040 kB
物理剩余内存=MemFree: 1044856 kB
内核版本与操作系统=Linux version 4.18.0-80.el8.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 8.2.1 20180905 (Red Hat 8.2.1-3) (GCC)) #1 SMP Tue Jun 4 09:19:46 UTC 2019
cpu数目=4
一个cpu上的核数=cpu cores : 1
一个cpu上的核数和cpu型号和每个核的频率= 4 Intel(R) Core(TM) i5-2400 CPU @ 3.10GHz
还是剩100多M， glustefd服务加一个pod， 物理内存超过了3G， 造成213上的gluster pod不断重启
[master@212-node shell]$ k get po -owide
NAME                             READY   STATUS    RESTARTS   AGE    IP              NODE       NOMINATED NODE   READINESS GATES
deploy-heketi-65d9c564d6-69dq6   1/1     Running   0          4h     192.168.135.1   217-node   <none>           <none>
glusterfs-pfqdr                  0/1     Running   61         4h2m   192.168.0.213   213-node   <none>           <none>
glusterfs-vhbgn                  1/1     Running   0          4h2m   192.168.0.217   217-node   <none>           <none>
[master@212-node shell]$ k get po -owide
NAME                             READY   STATUS    RESTARTS   AGE    IP              NODE       NOMINATED NODE   READINESS GATES
deploy-heketi-65d9c564d6-69dq6   1/1     Running   0          4h2m   192.168.135.1   217-node   <none>           <none>
glusterfs-pfqdr                  0/1     Running   62         4h4m   192.168.0.213   213-node   <none>           <none>
glusterfs-vhbgn                  1/1     Running   0          4h4m   192.168.0.217   217-node   <none>           <none>


[master@212-node shell]$ k get po -owide
NAME                             READY   STATUS             RESTARTS   AGE    IP              NODE       NOMINATED NODE   READINESS GATES
deploy-heketi-65d9c564d6-69dq6   1/1     Running            0          4h3m   192.168.135.1   217-node   <none>           <none>
glusterfs-pfqdr                  0/1     CrashLoopBackOff   62         4h5m   192.168.0.213   213-node   <none>           <none>
glusterfs-vhbgn                  1/1     Running            0          4h5m   192.168.0.217   217-node   <none>           <none>





[user9@217-node shell]$ ./catcpumem.sh
物理总内存=MemTotal: 3848804 kB
物理剩余内存=MemFree: 550204 kB
内核版本与操作系统=Linux version 4.18.0-80.el8.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 8.2.1 20180905 (Red Hat 8.2.1-3) (GCC)) #1 SMP Tue Jun 4 09:19:46 UTC 2019
cpu数目=4
一个cpu上的核数=cpu cores : 1
一个cpu上的核数和cpu型号和每个核的频率= 4 Intel(R) Celeron(R) CPU J3455 @ 1.50GHz


Events:
  Type     Reason     Age                      From               Message
  ----     ------     ----                     ----               -------
  Warning  BackOff    5m51s (x538 over 3h27m)  kubelet, 213-node  Back-off restarting failed container
  Warning  Unhealthy  60s (x558 over 4h)       kubelet, 213-node  (combined from similar events): Readiness probe failed: ● glusterd.service - GlusterFS, a clustered file-system server
   Loaded: loaded (/usr/lib/systemd/system/glusterd.service; enabled; vendor preset: disabled)
   Active: failed (Result: exit-code) since Sat 2020-01-11 08:56:58 UTC; 1min 58s ago

Jan 11 08:56:58 213-node systemd[1]: Starting GlusterFS, a clustered file-system server...
Jan 11 08:56:58 213-node systemd[1]: glusterd.service: control process exited, code=exited status=1
Jan 11 08:56:58 213-node systemd[1]: Failed to start GlusterFS, a clustered file-system server.
Jan 11 08:56:58 213-node systemd[1]: Unit glusterd.service entered failed state.
Jan 11 08:56:58 213-node systemd[1]: glusterd.service failed.


找另一个可以4G内存的主机， 加入到集群
[user1@141-node ~]$ sudo systemctl start glusterfsd.service
[user1@141-node ~]$ sudo systemctl status glusterfsd.service
● glusterfsd.service - GlusterFS brick processes (stopping only)
   Loaded: loaded (/usr/lib/systemd/system/glusterfsd.service; disabled; vendor preset: disabled)
   Active: active (exited) since Sat 2020-01-11 04:09:24 EST; 5s ago
  Process: 8473 ExecStart=/bin/true (code=exited, status=0/SUCCESS)
 Main PID: 8473 (code=exited, status=0/SUCCESS)

1月 11 04:09:24 219-node systemd[1]: Starting GlusterFS brick processes (stopping only)...
1月 11 04:09:24 219-node systemd[1]: Started GlusterFS brick processes (stopping only).

因为集群没加入

[master@212-node shell]$ kubectl label node 219-node  storagenode=glusterfs
Error from server (NotFound): nodes "219-node" not found

所有被比较的node都会被调度上
[master@212-node k8s_yaml]$ kubectl label node 219-node  storagenode=glusterfs


虽然213-node 被重新标记了， 但是213-pod delete后， 还是会有Pod调度到这个node上
[master@212-node k8s_yaml]$ kubectl label node 213-node  node=kube-node
node/213-node labeled
[master@212-node k8s_yaml]$ k get po
NAME                             READY   STATUS    RESTARTS   AGE
deploy-heketi-65d9c564d6-69dq6   1/1     Running   0          4h27m
glusterfs-4p699                  0/1     Running   4          8m33s
glusterfs-pfqdr                  0/1     Running   67         4h28m
glusterfs-vhbgn                  1/1     Running   0          4h28m
[master@212-node k8s_yaml]$ k get po
NAME                             READY   STATUS    RESTARTS   AGE
deploy-heketi-65d9c564d6-69dq6   1/1     Running   0          4h27m
glusterfs-4p699                  0/1     Running   4          8m36s
glusterfs-pfqdr                  0/1     Running   67         4h28m
glusterfs-vhbgn                  1/1     Running   0          4h28m
[master@212-node k8s_yaml]$ k delete po glusterfs-pfqdr
pod "glusterfs-pfqdr" deleted
[master@212-node k8s_yaml]$ k get po
NAME                             READY   STATUS    RESTARTS   AGE
deploy-heketi-65d9c564d6-69dq6   1/1     Running   0          4h29m
glusterfs-4p699                  0/1     Running   5          10m
glusterfs-smmcv                  0/1     Running   0          25s
glusterfs-vhbgn                  1/1     Running   0          4h30m
[master@212-node k8s_yaml]$ k get po -owide
NAME                             READY   STATUS    RESTARTS   AGE     IP              NODE       NOMINATED NODE   READINESS GATES
deploy-heketi-65d9c564d6-69dq6   1/1     Running   0          4h29m   192.168.135.1   217-node   <none>           <none>
glusterfs-4p699                  0/1     Running   5          10m     192.168.0.219   219-node   <none>           <none>
glusterfs-smmcv                  0/1     Running   0          33s     192.168.0.213   213-node   <none>           <none>
glusterfs-vhbgn                  1/1     Running   0          4h30m   192.168.0.217   217-node   <none>           <none>
原因是：
这里只是都213-node再次标记， 并没有取消原标记， 有了双重标记
[master@212-node k8s_yaml]$ kubectl get node  -l "node=kube-node"
NAME       STATUS   ROLES    AGE     VERSION
213-node   Ready    <none>   4h46m   v1.17.0
[master@212-node k8s_yaml]$ kubectl get node  -l "storagenode=glusterfs"
NAME       STATUS   ROLES    AGE     VERSION
213-node   Ready    <none>   4h47m   v1.17.0
217-node   Ready    <none>   4h45m   v1.17.0
219-node   Ready    <none>   12m     v1.17.0
