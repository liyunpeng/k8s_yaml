sudo wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo
-------------------
1. 添加repo文件
[centos5@68-node ~]$ cat /etc/yum.repos.d/glusterfs.repo
[glusterfs-rhel8]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes.
baseurl=https://download.gluster.org/pub/gluster/glusterfs/6/LATEST/RHEL/el-$releasever/$basearch/
enabled=1
skip_if_unavailable=1
gpgcheck=1
gpgkey=https://download.gluster.org/pub/gluster/glusterfs/6/rsa.pub

[glusterfs-noarch-rhel8]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes.
baseurl=https://download.gluster.org/pub/gluster/glusterfs/6/LATEST/RHEL/el-$releasever/noarch
enabled=1
skip_if_unavailable=1
gpgcheck=1
gpgkey=https://download.gluster.org/pub/gluster/glusterfs/6/rsa.pub

[glusterfs-source-rhel8]
[glusterfs-source-rhel8]
name=GlusterFS is a clustered file-system capable of scaling to several petabytes. - Source
baseurl=https://download.gluster.org/pub/gluster/glusterfs/6/LATEST/RHEL/el-$releasever/SRPMS
enabled=0
skip_if_unavailable=1
gpgcheck=1
gpgkey=https://download.gluster.org/pub/gluster/glusterfs/6/rsa.pub

---------------------------------
  sudo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

  您可以通过执行 'dnf clean packages' 删除软件包缓存。
  错误：下载软件包出错
    Cannot download glusterfs-fuse-6.6-1.el8.x86_64.rpm: All mirrors were tried
[master@212-node ~]$ dnf clean packages
Repository epel is listed more than once in the configuration
Repository epel-debuginfo is listed more than once in the configuration
Repository epel-source is listed more than once in the configuration
Repository glusterfs-noarch-rhel8 is listed more than once in the configuration
Repository glusterfs-rhel8 is listed more than once in the configuration
Repository glusterfs-source-rhel8 is listed more than once in the configuration
0 文件已删除

[master@212-node glusterfs]$ systemctl status firewalld.service
● firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)
   Active: inactive (dead)
     Docs: man:firewalld(1)
[master@212-node glusterfs]$ systemctl stop firewalld.service
==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ====
停止“firewalld.service”需要认证。
Authenticating as: root
Password:
==== AUTHENTICATION COMPLETE ====
[master@212-node glusterfs]$ systemctl status firewalld.service
● firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)
   Active: inactive (dead)
     Docs: man:firewalld(1)

[master@212-node glusterfs]$ systemctl status selinux-autorelabel.service
● selinux-autorelabel.service - Relabel all filesystems
   Loaded: loaded (/usr/lib/systemd/system/selinux-autorelabel.service; static; vendor preset: disabled)
   Active: inactive (dead)

======================================
[centos2@213-node glusterfs]$ sudo gluster peer probe 192.168.0.213
[sudo] centos2 的密码：
peer probe: success. Probe on localhost not needed
[centos2@213-node glusterfs]$ sudo gluster peer probe 192.168.0.212
peer probe: success. Host 192.168.0.212 port 24007 already in peer list
[centos2@213-node glusterfs]$ sudo gluster peer probe 192.168.0.215
peer probe: failed: Probe returned with 传输端点尚未连接
[centos2@213-node glusterfs]$ sudo gluster peer probe 192.168.0.212
peer probe: success. Host 192.168.0.212 port 24007 already in peer list
[centos2@213-node glusterfs]$ sudo gluster peer status
Number of Peers: 1

Hostname: 192.168.0.212
Uuid: db4004ba-dd3a-4646-ae63-c731088d762b
State: Accepted peer request (Connected)

[centos2@213-node glusterfs]$ sudo gluster pool list
UUID                                    Hostname        State
db4004ba-dd3a-4646-ae63-c731088d762b    192.168.0.212   Connected
31111f45-b237-4be3-8c9f-aa85b3ba825e    localhost       Connected

=================================
[master@212-node glusterfs]$ sudo yum -y install glusterfs-client
[sudo] master 的密码：
Repository epel is listed more than once in the configuration
Repository epel-debuginfo is listed more than once in the configuration
Repository epel-source is listed more than once in the configuration
Repository glusterfs-noarch-rhel8 is listed more than once in the configuration
Repository glusterfs-rhel8 is listed more than once in the configuration
Repository glusterfs-source-rhel8 is listed more than once in the configuration
上次元数据过期检查：0:11:55 前，执行于 2020年01月05日 星期日 09时10分15秒。
Package glusterfs-fuse-6.6-1.el8.x86_64 is already installed.


=============================================

[centos2@213-node glusterfs]$ sudo gluster pool list
UUID                                    Hostname        State
db4004ba-dd3a-4646-ae63-c731088d762b    192.168.0.212   Connected
31111f45-b237-4be3-8c9f-aa85b3ba825e    localhost       Connected
[centos2@213-node glusterfs]$ sudo gluster volume create test01 replica 2 192.168^Cdata/brick1/volume0 213-node:/data/brick1/volume0
[centos2@213-node glusterfs]$ sudo gluster probe peer 212-node
unrecognized word: probe (position 0)

 Usage: gluster [options] <help> <peer> <pool> <volume>
 Options:
 --help  Shows the help information
 --version  Shows the version
 --print-logdir  Shows the log directory
 --print-statedumpdir Shows the state dump directory

[centos2@213-node glusterfs]$ sudo gluster peer  probe 212-node
peer probe: success. Host 212-node port 24007 already in peer list
[centos2@213-node glusterfs]$ sudo gluster volume create test01 replica 2 212-node:/data/brick1/volume0 213-node:/data/brick1/volume0
Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.
Do you still want to continue?
 (y/n) y
volume create: test01: failed: Host 212-node is not in 'Peer in Cluster' state
[centos2@213-node glusterfs]$ sudo gluster volume create test01 replica 1 213-node:/data/brick1/volume0                          replica count should be greater than 1

Usage:
volume create <NEW-VOLNAME> [stripe <COUNT>] [replica <COUNT> [arbiter <COUNT>]] [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport <tcp|rdma|tcp,rdma>] <NEW-BRICK>... [force]

[centos2@213-node glusterfs]$ sudo gluster volume create test01 replica 2 213-node:/data/brick1/volume0
Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.
Do you still want to continue?
 (y/n) y
number of bricks is not a multiple of replica count

Usage:
volume create <NEW-VOLNAME> [stripe <COUNT>] [replica <COUNT> [arbiter <COUNT>]] [disperse [<COUNT>]] [disperse-data <COUNT>] [redundancy <COUNT>] [transport <tcp|rdma|tcp,rdma>] <NEW-BRICK>... [force]

[centos2@213-node glusterfs]$ sudo gluster volume create test01 replica 2 192.168.0.212:/data/brick1/volume0 213-node:/data/brick1/volume0
Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.
Do you still want to continue?
 (y/n) y
volume create: test01: failed: Host 192.168.0.212 is not in 'Peer in Cluster' state
[centos2@213-node glusterfs]$ sudo gluster peer status
Number of Peers: 1

启动gluster， 先把/etc/hosts， 名字设置好：
213上操作：
[centos2@213-node glusterfs]$ cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.0.212   212-node
[centos2@213-node glusterfs]$  sudo systemctl restart glusterd.service

212上操作：
[master@212-node glusterfs]$ cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.0.213   213-node
[master@212-node glusterfs]$ sudo systemctl restart gluster
glusterd.service                glusterfsd.service              glusterfssharedstorage.service


==========================================

[master@212-node glusterfs]$ sudo gluster pool list
UUID                                    Hostname        State
31111f45-b237-4be3-8c9f-aa85b3ba825e    213-node        Connected
db4004ba-dd3a-4646-ae63-c731088d762b    localhost       Connected



[centos2@213-node glusterfs]$ sudo gluster pool list
UUID                                    Hostname        State
db4004ba-dd3a-4646-ae63-c731088d762b    212-node        Connected
31111f45-b237-4be3-8c9f-aa85b3ba825e    localhost       Connected


-=======================


[centos2@213-node glusterfs]$ sudo gluster volume create dis_vol 212-node:/data/brick1/volume0 213-node:/data/brick1/volume0
volume create: dis_vol: failed: The brick 213-node:/data/brick1/volume0 is being created in the root partition. It is recommended that you don't use the system's root partition for storage backend. Or use 'force' at the end of the command if you want to override this behavior.
[centos2@213-node glusterfs]$ sudo gluster volume create dis_vol 212-node:/data/brick1/volume0 213-node:/data/brick1/volume0 force
volume create: dis_vol: success: please start the volume to access data

[centos2@213-node glusterfs]$ sudo  gluster volume  info dis_vol

Volume Name: dis_vol
Type: Distribute
Volume ID: d9c3278e-9766-4590-a0b7-08f83277d44b
Status: Created
Snapshot Count: 0
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: 212-node:/data/brick1/volume0
Brick2: 213-node:/data/brick1/volume0
Options Reconfigured:
transport.address-family: inet
nfs.disable: on


=============================

[centos2@213-node volume0]$ sudo gluster volume
unrecognized command

 Usage: gluster [options] <help> <peer> <pool> <volume>
 Options:
 --help  Shows the help information
 --version  Shows the version
 --print-logdir  Shows the log directory
 --print-statedumpdir Shows the state dump directory

[centos2@213-node volume0]$ sudo gluster volume status
Status of volume: dis_vol
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick 212-node:/data/brick1/volume0         49152     0          Y       41101
Brick 213-node:/data/brick1/volume0         49152     0          Y       40785

Task Status of Volume dis_vol



[master@212-node mnt]$ sudo  mount -t glusterfs 213-node:dis_vol ./a

===================
[centos2@213-node rep]$ sudo gluster volume create rep_vol replica 2 213-node:/data/brick1/rep 212-node:/data/brick1/rep force
volume create: rep_vol: success: please start the volume to access data
[centos2@213-node rep]$ gluster volume
ERROR: failed to create logfile "/var/log/glusterfs/cli.log" (权限不够)
ERROR: failed to open logfile /var/log/glusterfs/cli.log
[centos2@213-node rep]$ gluster volume status
ERROR: failed to create logfile "/var/log/glusterfs/cli.log" (权限不够)
ERROR: failed to open logfile /var/log/glusterfs/cli.log
[centos2@213-node rep]$ sudo gluster volume status
Status of volume: dis_vol
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick 212-node:/data/brick1/volume0         49152     0          Y       41101
Brick 213-node:/data/brick1/volume0         49152     0          Y       40785

Task Status of Volume dis_vol
------------------------------------------------------------------------------
There are no active volume tasks

Volume rep_vol is not started

[centos2@213-node rep]$ sudo gluster start rep_vol
unrecognized word: start (position 0)

 Usage: gluster [options] <help> <peer> <pool> <volume>
 Options:
 --help  Shows the help information
 --version  Shows the version
 --print-logdir  Shows the log directory
 --print-statedumpdir Shows the state dump directory

[centos2@213-node rep]$ sudo gluster volume start rep_vol
volume start: rep_vol: success
[centos2@213-node rep]$ sudo gluster volume status
Status of volume: dis_vol
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick 212-node:/data/brick1/volume0         49152     0          Y       41101
Brick 213-node:/data/brick1/volume0         49152     0          Y       40785

Task Status of Volume dis_vol
------------------------------------------------------------------------------
There are no active volume tasks

Status of volume: rep_vol
Gluster process                             TCP Port  RDMA Port  Online  Pid
------------------------------------------------------------------------------
Brick 213-node:/data/brick1/rep             49153     0          Y       57600
Brick 212-node:/data/brick1/rep             49153     0          Y       57827
Self-heal Daemon on localhost               N/A       N/A        Y       57621
Self-heal Daemon on 212-node                N/A       N/A        Y       57848

Task Status of Volume rep_vol
------------------------------------------------------------------------------
There are no active volume tasks


复制卷有同步复制功能， 多个网路存储盘， 同时自动复制
启用volume 之后， 才能挂载volume.


[master@212-node mnt]$ sudo  mount -t glusterfs 213-node:rep_vol ./rep
[sudo] master 的密码：
[master@212-node mnt]$ cd rep/
[master@212-node rep]$ touch fffffffffffffff
[master@212-node rep]$ ls /data/brick1/
rep/     volume0/
[master@212-node rep]$ ls /data/brick1/rep/
fffffffffffffff  .glusterfs/
